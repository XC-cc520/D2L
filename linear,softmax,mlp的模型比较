#深度学习linear,softmax,mlp的模型比较
'''比较三种模型的代码，找出他们的异同'''
1.生成数据集，调用库
#linear
    import numpy as np                              #调用numpy以np命名
	import torch
	from torch.utils import data                    #从torch.utils中调用data
	from d2l import torch as d2l                    #从d2l中调用torch以d2l命名
#softnax
    import torch
	from torch import nn                            #nn是神经网络的缩写nn.linear(P67)
	from d2l import torch as d2l
#mlp
	import torch
	from torch import nn
	from d2l import torch as d2l

2.定义变量net：Sequential
#linear
	from torch import nn
	net = nn.Sequential(nn.Linear(2, 1))
#softmax
    net = nn.Sequential(nn.Flatten(), nn.Linear(784, 10))  #Flatten二维展平层

#mlp
    net = nn.Sequential(nn.Flatten(),
          nn.Linear(784, 256),
          nn.ReLU(),  
                                                  #ReLU是激活函数，它可以抛弃掉一些不用的值（当输入为负时，导数为0，输入为正时时，导数为1）；这也mlp区别与其他两种的区别之一
          nn.Linear(256, 10))
                                                    #从这里可以看到mlp有三层，而softmaax只有两层

3.权重初始化
#linear
    net[0].weight.data.normal_(0, 0.01)
	net[0].bias.data.fill_(0)
#softmax
    def init_weights(m):
    	if type(m) == nn.Linear:
        	nn.init.normal_(m.weight, std=0.01)
    net.apply(init_weights);
#mlp
	def init_weights(m):
	    if type(m) == nn.Linear:
	        nn.init.normal_(m.weight, std=0.01)

	net.apply(init_weights);

4..设置超参数
#linear                                       
	batch_size = 10                                       #超参数，训练轮数，学习率
    num_epochs = 3
	lr=0.03
#softmax
    batch_size = 256
    num_epochs = 10
    lr=0.1
 #mlp
	batch_size, lr, num_epochs = 256, 0.1, 10

5.定义损失函数
#linear
	loss = nn.MSELoss()
#softmax
	loss = nn.CrossEntropyLoss(reduction='none')
#mlp
	loss = nn.CrossEntropyLoss(reduction='none')

6.优化算法
#linear
	trainer = torch.optim.SGD(net.parameters(), lr=0.03)   #使用SGD优化算法（小批量求导，梯度下降）
#softmax
	trainer = torch.optim.SGD(net.parameters(), lr=0.1)
#mlp
	trainer = torch.optim.SGD(net.parameters(), lr=lr)

7.训练
#linear
    num_epochs = 3
	for epoch in range(num_epochs):                                      #外层循环遍历每一轮训练（epoch）
    for i,(X, y) in enumerate (data_iter()):                             #内层遍历
        l = loss(net(X) ,y)
        trainer.clear_grad()
        l.backward()
        trainer.step()
    l = loss(net(features), labels)                                       #计算当前损失
    print(f'epoch {epoch + 1},'f'loss {l}')
#softmax
    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)       #载入数据
    d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)  #变量，训练值，测试值，损失函数，训练轮数
#mlp
	train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
	d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)


'''
异:
linear,softmax没有非线性能力，而mlp有非线性
mlp是多个全连接层模型
softmax和mlp都包含了linear,都使用了linear
mlp有三层，而softmaax只有两层,linear是简单的线性模型
同：
都使用nn,loss,SGD,nn.Sequential
'''

'''
linear怎样读取数据（迭代器）
#linear
	def load_array(data_arrays, batch_size, is_train=True):  #@save
	    """构造一个PyTorch数据迭代器"""
	    dataset = data.TensorDataset(*data_arrays)
	    return data.DataLoader(dataset, batch_size, shuffle=is_train)
	batch_size = 10
	data_iter = load_array((features, labels), batch_size)
'''
